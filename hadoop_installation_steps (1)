one@one:~$ sudo su

root@one:/home/one# cd

root@one:~# sudo apt-get update

sudo apt-get install default-jdk

root@one:~# java -version

root@one:~# sudo apt-get install ssh

error : E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?

root@one:~# sudo apt-get update

root@one:~# ssh-keygen -t rsa -P ""


root@one:~# cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys

root@one:~# ssh localhost

root@one:~# cd /home/one/Download

root@one:/home/one/Downloads# tar xvzf hadoop-2.6.0.tar.gz

root@one:/home/one/Downloads# cd

root@one:~# sudo mkdir -p /usr/local/hadoop

root@one:~# cd /home/one/Downloads/hadoop-2.6.0/

root@one:/home/one/Downloads/hadoop-2.6.0# sudo mv * /usr/local/hadoop

root@one:/home/one/Downloads/hadoop-2.6.0# sudo chown -R root /usr/local/hadoop

nano ~/.bashrc

#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-8-oracle
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"
export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib"
#HADOOP VARIABLES END


source ~/.bashrc

nano /usr/local/hadoop/etc/hadoop/hadoop-env.sh

export JAVA_HOME=/usr/lib/jvm/java-8-oracle

sudo mkdir -p /app/hadoop/tmp

sudo chown root /app/hadoop/tmp

nano /usr/local/hadoop/etc/hadoop/core-site.xml

<configuration>
 <property>
  <name>hadoop.tmp.dir</name>
  <value>/app/hadoop/tmp</value>
  <description>A base for other temporary directories.</description>
 </property>

 <property>
  <name>fs.default.name</name>
  <value>hdfs://localhost:54310</value>
  <description>The name of the default file system.  A URI whose
  scheme and authority determine the FileSystem implementation.  The
  uri's scheme determines the config property (fs.SCHEME.impl) naming
  the FileSystem implementation class.  The uri's authority is used to
  determine the host, port, etc. for a filesystem.</description>
 </property>
</configuration>

cp /usr/local/hadoop/etc/hadoop/mapred-site.xml.template /usr/local/hadoop/etc/hadoop/mapred-site.xml

nano /usr/local/hadoop/etc/hadoop/mapred-site.xml

<configuration>
 <property>
  <name>mapred.job.tracker</name>
  <value>localhost:54311</value>
  <description>The host and port that the MapReduce job tracker runs
  at.  If "local", then jobs are run in-process as a single map
  and reduce task.
  </description>
 </property>
</configuration>

sudo mkdir -p /usr/local/hadoop_store/hdfs/namenode

sudo mkdir -p /usr/local/hadoop_store/hdfs/datanode

sudo chown -R root /usr/local/hadoop_store

nano /usr/local/hadoop/etc/hadoop/hdfs-site.xml

<configuration>
 <property>
  <name>dfs.replication</name>
  <value>1</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
 </property>
 <property>
   <name>dfs.namenode.name.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/namenode</value>
 </property>
 <property>
   <name>dfs.datanode.data.dir</name>
   <value>file:/usr/local/hadoop_store/hdfs/datanode</value>
 </property>
</configuration>


hadoop namenode -format


start-all.sh


